{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, RepeatVector, Embedding\n",
    "from keras.layers.core import Activation, Dense, Masking\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(object):\n",
    "    \"\"\"docstring for Autoencoder\"\"\"\n",
    "    def __init__(self):\n",
    "        self.nb_epoch = 10  # epochs\n",
    "        self.batch_size = 256\n",
    "        self.shuffle = True\n",
    "        self.validation_split = 0.05\n",
    "        self.optimizer = 'adadelta'\n",
    "        self.loss = 'mse'\n",
    "\n",
    "    # 处理fixed-length sequence的model\n",
    "    def model(self, codeLayerType, inputDim, codeDim):\n",
    "        self.codeLayerType = codeLayerType\n",
    "        assert len(codeDim) > 0\n",
    "\n",
    "        if self.codeLayerType == 'lstm':\n",
    "            assert len(inputDim) == 2\n",
    "            inputData = Input(shape=(inputDim[0],inputDim[1]))\n",
    "\n",
    "            if len(codeDim) == 1:\n",
    "                encoded = LSTM(codeDim[0])(inputData)\n",
    "                decoded = RepeatVector(inputDim[0])(encoded)\n",
    "            elif len(codeDim) > 1:\n",
    "                encoded = inputData\n",
    "                for i, units in enumerate(codeDim):\n",
    "                    if i == len(codeDim) - 1:\n",
    "                        encoded = LSTM(units)(encoded)\n",
    "                        continue\n",
    "                    encoded = LSTM(units, return_sequences=True)(encoded)\n",
    "\n",
    "                for i, units in enumerate(reversed(codeDim)): \n",
    "                    if i == 1:\n",
    "                        decoded = LSTM(units, return_sequences=True)(RepeatVector(inputDim[0])(encoded))\n",
    "                    elif i > 1: \n",
    "                        decoded = LSTM(units, return_sequences=True)(decoded)\n",
    "            else: \n",
    "                raise ValueError(\"The codDim must be over 0.\")\n",
    "\n",
    "            decoded = LSTM(inputDim[-1], return_sequences=True)(decoded)\n",
    "            self.model = Model(inputData, decoded)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def modelMasking(self, codeLayerType, inputDim, codeDim):\n",
    "        self.codeLayerType = codeLayerType\n",
    "        assert len(codeDim) > 0\n",
    "\n",
    "        if self.codeLayerType == 'lstm':\n",
    "            assert len(inputDim) == 2\n",
    "            inputData = Input(shape=(inputDim[0],inputDim[1]))\n",
    "            mask = Masking(mask_value=0.)(inputData)\n",
    "            if len(codeDim) == 1:\n",
    "                encoded = LSTM(codeDim[0])(mask)\n",
    "                decoded = RepeatVector(inputDim[0])(encoded)\n",
    "            elif len(codeDim) > 1:\n",
    "                encoded = mask\n",
    "                for i, units in enumerate(codeDim):\n",
    "                    if i == len(codeDim) - 1:\n",
    "                        encoded = LSTM(units)(encoded)\n",
    "                        continue\n",
    "                    encoded = LSTM(units, return_sequences=True)(encoded)\n",
    "    \n",
    "                for i, units in enumerate(reversed(codeDim)):\n",
    "                    if i == 1:\n",
    "                        decoded = LSTM(units, return_sequences=True)(RepeatVector(inputDim[0])(encoded))\n",
    "                    elif i > 1: \n",
    "                        decoded = LSTM(units, return_sequences=True)(decoded)\n",
    "            else: \n",
    "                raise ValueError(\"The codDim must be over 0.\")\n",
    "\n",
    "            decoded = LSTM(inputDim[-1], return_sequences=True)(decoded)\n",
    "            self.model = Model(inputData, decoded)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "\n",
    "    def compile(self, *args):\n",
    "        if len(args) == 0:\n",
    "            self.model.compile(optimizer=self.optimizer, loss=self.loss)\n",
    "        elif len(args) == 1:\n",
    "            if args[0] == 'temporal':\n",
    "                self.sample_weight_mode = args[0]\n",
    "                self.model.compile(optimizer=self.optimizer, loss=self.loss, sample_weight_mode=self.sample_weight_mode)\n",
    "            elif args[0] == 'customFunction':\n",
    "                self.model.compile(optimizer=self.optimizer, loss= self.weighted_vector_mse)\n",
    "            else: \n",
    "                raise ValueError(\"Invalid maskType, please input 'sampleWeights' or 'customFunction'\")\n",
    "        else: \n",
    "            raise ValueError(\"argument # must be 0 or 1.\")\n",
    "\n",
    "\n",
    "    def fit(self, *args):\n",
    "        if len(args) == 2:\t\n",
    "            if args[1] == 'nor':\n",
    "                self.model.fit(args[0],\n",
    "                               args[0],\n",
    "                               epochs=self.nb_epoch, \n",
    "                               batch_size=self.batch_size, \n",
    "                               shuffle=self.shuffle, \n",
    "                               validation_split=self.validation_split)\n",
    "            elif args[1] == 'rev':\n",
    "                self.model.fit(args[0], \n",
    "                               np.flip(args[0], 1), \n",
    "                               epochs=self.nb_epoch, \n",
    "                               batch_size=self.batch_size, \n",
    "                               shuffle=self.shuffle, \n",
    "                               validation_split=self.validation_split)\n",
    "            else: \n",
    "                raise ValueError(\"decoding sequence type: 'normal' or 'reverse'.\")\n",
    "\n",
    "        elif len(args) == 3:\n",
    "            self.sampleWeights = args[2]\n",
    "            if args[1] == 'nor':\n",
    "                self.model.fit(args[0],\n",
    "                               args[0],\n",
    "                               epochs=self.nb_epoch, \n",
    "                               batch_size=self.batch_size, \n",
    "                               shuffle=self.shuffle, \n",
    "                               validation_split=self.validation_split, \n",
    "                               sample_weight=self.sampleWeights)\n",
    "            elif args[1] == 'rev':\n",
    "                self.model.fit(args[0],\n",
    "                               np.flip(args[0], 1), \n",
    "                               epochs=self.nb_epoch, \n",
    "                               batch_size=self.batch_size, \n",
    "                               shuffle=self.shuffle, \n",
    "                               validation_split=self.validation_split,\n",
    "                               sample_weight=self.sampleWeights)\n",
    "            else: \n",
    "                raise ValueError(\"Please input, 'data', 'nor' or 'rev', 'sample_weights'\")\n",
    "\n",
    "    def predict(self, data):\n",
    "        return self.model.predict(data)\n",
    "\n",
    "    def weighted_vector_mse(self, y_true, y_pred):\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "        weight = tf.ceil(self.y_true) # 向上取整\n",
    "        loss = tf.square(weight * (self.y_true - self.y_pred)) \n",
    "        return tf.reduce_mean(tf.reduce_sum(loss, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Autoencoder(object):\n",
    "    \"\"\"docstring for LSTM_Autoencoder\"\"\"\n",
    "    def __init__(self, input_dim, time_step, hidden_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.time_step = time_step\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.autoencoder = Autoencoder()\n",
    "        self.autoencoder.modelMasking('lstm', [self.time_step, self.input_dim], self.hidden_dim)\n",
    "\n",
    "    def compile(self):\n",
    "        self.autoencoder.compile('temporal')\n",
    "\n",
    "    def fit(self, data, weights):\n",
    "        self.autoencoder.fit(data, 'rev', weights)\n",
    "\n",
    "    def get_hidden_layer_last_step(self):\n",
    "        self.hidden_representation = Sequential()\n",
    "        self.hidden_representation.add(self.autoencoder.model.layers[0])\n",
    "        self.hidden_representation.add(self.autoencoder.model.layers[1])\n",
    "        self.hidden_representation.add(self.autoencoder.model.layers[2])\n",
    "\n",
    "    def get_hidden_layer_sequence(self):\n",
    "        inputData = Input(shape=(self.time_step, self.input_dim))\n",
    "        mask = Masking(mask_value=0.)(inputData)\n",
    "        encoded = LSTM(self.hidden_dim[0], return_sequences=True, weights=self.autoencoder.model.layers[2].get_weights())(mask)\n",
    "        self.hidden_representation = Model(inputData, encoded)\n",
    "\n",
    "    def get_hidden_representation(self, data):\n",
    "        return self.hidden_representation.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_shuffle(X):\n",
    "    n_samples = len(X)\n",
    "    s = np.arange(n_samples)\n",
    "    np.random.shuffle(s)\n",
    "    return np.array(X[s])\n",
    "\n",
    "def seq_padding(sample_sequence, max_length, padding_type):\n",
    "    return pad_sequences(sample_sequence, maxlen=max_length, dtype='float', padding=padding_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_hid_repre(fea_dim, hid_dim, fix_or_var, step_length):\n",
    "    \n",
    "    \"\"\"\n",
    "    :param fea_dim: input dimension of LSTM-AE model\n",
    "    :param hid_dim: output dimension of hidden representation\n",
    "    :param fix_or_var:  editing sequence is fixed-length or variant-length.\n",
    "    :return: fixed-length hidden representation of editing sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    # 定长序列处理\n",
    "    if fix_or_var == 1:\n",
    "        # Load data\n",
    "        x_ben = np.load('data/wiki/', encoding='bytes') # fixed-length sequence\n",
    "        x_van = np.load('data/wiki/', encoding='bytes')\n",
    "        print(x_ben.shape, x_van.shape)\n",
    "        x_ben = sample_shuffle(x_ben)[0:6000]\n",
    "        x_van = sample_shuffle(x_van)[0:3000]\n",
    "        train_ben = x_ben[0:3000]\n",
    "\n",
    "        # Fit Model\n",
    "        timesteps = 20\n",
    "        input_dim = fea_dim\n",
    "        \n",
    "        autoencoder = Autoencoder()\n",
    "        autoencoder.model('lstm', [timesteps, input_dim], hid_dim)\n",
    "        autoencoder.compile()\n",
    "        autoencoder.fit(train_ben, \"rev\")\n",
    "\n",
    "        hidModel = Sequential()\n",
    "        hidModel.add(autoencoder.model.layers[0])\n",
    "        hidModel.add(autoencoder.model.layers[1])\n",
    "\n",
    "        ben_hid_emd = hidModel.predict(x_ben)\n",
    "        van_hid_emd = hidModel.predict(x_van)\n",
    "\n",
    "        # store data\n",
    "#         np.save(\"data/wiki/ben_hid_emd_20_%s_%s\" % (fea_dim, hid_dim[0]), ben_hid_emd)\n",
    "#         np.save(\"data/wiki/van_hid_emd_20_%s_%s\" % (fea_dim, hid_dim[0]), van_hid_emd)\n",
    "\n",
    "    # 变长序列处理\n",
    "    elif fix_or_var == 0:\n",
    "        if step_length == 20:\n",
    "            x_ben = np.load('data/wiki/',encoding='bytes')\n",
    "            x_van = np.load('data/wiki/',encoding='bytes')\n",
    "            x_ben = sample_shuffle(x_ben)  \n",
    "            x_van = sample_shuffle(x_van)  \n",
    "            train_ben = x_ben[0:10000]\n",
    "\n",
    "            sampleWeights = list()\n",
    "            for e in train_ben:\n",
    "                sampleWeights.append(np.ones(len(e)))\n",
    "\n",
    "            train_ben_P = pad_sequences(train_ben, maxlen=20, dtype='float')\n",
    "            x_ben_P = pad_sequences(x_ben, maxlen=20, dtype='float')\n",
    "            x_van_P = pad_sequences(x_van, maxlen=20, dtype='float')\n",
    "\n",
    "            # decoding sequence is reversed\n",
    "            sampleWeights = pad_sequences(sampleWeights, maxlen=20, dtype='float', padding='post')\n",
    "\n",
    "            timesteps = 20\n",
    "            input_dim = fea_dim\n",
    "            autoencoder = Autoencoder()\n",
    "            autoencoder.modelMasking('lstm', [timesteps, input_dim], hid_dim)\n",
    "            autoencoder.compile('temporal')\n",
    "            autoencoder.fit(train_ben_P, 'rev', sampleWeights)\n",
    "\n",
    "            hidModel = Sequential()\n",
    "            hidModel.add(autoencoder.model.layers[0])\n",
    "            hidModel.add(autoencoder.model.layers[1])\n",
    "            hidModel.add(autoencoder.model.layers[2])\n",
    "\n",
    "            ben_hid_emd = hidModel.predict(x_ben_P)\n",
    "            van_hid_emd = hidModel.predict(x_van_P)\n",
    "\n",
    "            # store data\n",
    "#             np.save(\"data/wiki/ben_hid_emd_mix_1_20_%s_%s\" % (fea_dim, hid_dim[0]), ben_hid_emd)\n",
    "#             np.save(\"data/wiki/val_hid_emd_mix_1_20_%s_%s\" % (fea_dim, hid_dim[0]), van_hid_emd)\n",
    "\n",
    "        elif step_length == 50:\n",
    "\n",
    "            x_ben = np.load('data/wiki/X_v8_4_50_Ben.npy', encoding='bytes')\n",
    "            x_van = np.load('data/wiki/X_v8_4_50_Van.npy', encoding='bytes')\n",
    "            x_ben = sample_shuffle(x_ben)\n",
    "            x_van = sample_shuffle(x_van)\n",
    "            train_ben = x_ben[0:7000]\n",
    "\n",
    "            sampleWeights = list()\n",
    "            for e in train_ben:\n",
    "                sampleWeights.append(np.ones(len(e)))\n",
    "\n",
    "            train_ben_P = pad_sequences(train_ben, maxlen=50, dtype='float')\n",
    "            x_ben_P = pad_sequences(x_ben, maxlen=50, dtype='float')\n",
    "            x_van_P = pad_sequences(x_van, maxlen=50, dtype='float')\n",
    "\n",
    "            # decoding sequence is reversed\n",
    "            sampleWeights = pad_sequences(sampleWeights, maxlen=50, dtype='float', padding='post')\n",
    "\n",
    "            timesteps = 50\n",
    "            input_dim = fea_dim\n",
    "            autoencoder = Autoencoder()\n",
    "            autoencoder.modelMasking('lstm', [timesteps, input_dim], hid_dim)\n",
    "            autoencoder.compile('temporal')\n",
    "            autoencoder.fit(train_ben_P, 'rev', sampleWeights)\n",
    "\n",
    "            hidModel = Sequential()\n",
    "            hidModel.add(autoencoder.model.layers[0])\n",
    "            hidModel.add(autoencoder.model.layers[1])\n",
    "            hidModel.add(autoencoder.model.layers[2])\n",
    "\n",
    "            ben_hid_emd = hidModel.predict(x_ben_P)\n",
    "            van_hid_emd = hidModel.predict(x_van_P)\n",
    "\n",
    "    return ben_hid_emd, van_hid_emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6650 samples, validate on 350 samples\n",
      "Epoch 1/10\n",
      "6650/6650 [==============================] - 9s 1ms/step - loss: 0.0628 - val_loss: 0.0463\n",
      "Epoch 2/10\n",
      "6650/6650 [==============================] - 8s 1ms/step - loss: 0.0441 - val_loss: 0.0423\n",
      "Epoch 3/10\n",
      "6650/6650 [==============================] - 8s 1ms/step - loss: 0.0407 - val_loss: 0.0399\n",
      "Epoch 4/10\n",
      "6650/6650 [==============================] - 8s 1ms/step - loss: 0.0390 - val_loss: 0.0390\n",
      "Epoch 5/10\n",
      "6650/6650 [==============================] - 8s 1ms/step - loss: 0.0377 - val_loss: 0.0377\n",
      "Epoch 6/10\n",
      "6650/6650 [==============================] - 8s 1ms/step - loss: 0.0368 - val_loss: 0.0366\n",
      "Epoch 7/10\n",
      "6650/6650 [==============================] - 8s 1ms/step - loss: 0.0363 - val_loss: 0.0365\n",
      "Epoch 8/10\n",
      "6650/6650 [==============================] - 8s 1ms/step - loss: 0.0357 - val_loss: 0.0357\n",
      "Epoch 9/10\n",
      "6650/6650 [==============================] - 8s 1ms/step - loss: 0.0354 - val_loss: 0.0353\n",
      "Epoch 10/10\n",
      "6650/6650 [==============================] - 8s 1ms/step - loss: 0.0350 - val_loss: 0.0351\n"
     ]
    }
   ],
   "source": [
    "ben_hid_emd, van_hid_emd =gen_hid_repre(fea_dim=8, hid_dim=[200], fix_or_var=0, step_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05116682, -0.07898297,  0.05688819, ...,  0.09775726,\n",
       "        -0.03702514, -0.1548021 ],\n",
       "       [ 0.06015232, -0.14864229,  0.08259501, ...,  0.28237966,\n",
       "        -0.15756416, -0.26166925],\n",
       "       [ 0.06606249, -0.10603029,  0.0479022 , ...,  0.1518603 ,\n",
       "        -0.08845998, -0.19822004],\n",
       "       ...,\n",
       "       [ 0.06719301, -0.10862384,  0.05089102, ...,  0.14848088,\n",
       "        -0.09090625, -0.19644676],\n",
       "       [-0.00965766, -0.15456763,  0.12148448, ...,  0.14247192,\n",
       "        -0.1070011 , -0.19472712],\n",
       "       [ 0.0288748 , -0.1534694 ,  0.17773622, ...,  0.18878806,\n",
       "        -0.10143352, -0.26629752]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ben_hid_emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
